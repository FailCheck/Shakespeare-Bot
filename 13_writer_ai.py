import torch
import torch.nn as nn
import numpy as np
import random

# ---------------------------------------------------------
# 1. ì„¤ì • (Config)
# ---------------------------------------------------------
# ìˆ˜ë°•ì„ í•œ ë²ˆì— ëª‡ ì¡°ê°ì”© ë„£ì„ ê²ƒì¸ê°€? (64ê°œì”© ë¬¶ì–´ì„œ í•™ìŠµ)
BATCH_SIZE = 64
# ìˆ˜ë°• í•œ ì¡°ê°ì˜ ê¸¸ì´ (ê¸€ì 100ê°œë¥¼ ë³´ê³  ë‹¤ìŒ ê¸€ì ë§ì¶”ê¸°)
SEQ_LENGTH = 100
# í•™ìŠµ íšŸìˆ˜ (ë°˜ë³µ í›ˆë ¨) - ë§ì´ í• ìˆ˜ë¡ ë˜‘ë˜‘í•´ì§
NUM_EPOCHS = 2000 
# ëª¨ë¸ì˜ ì¸µ ê°œìˆ˜ (ë” ê¹Šê²Œ ìŒ“ê¸°)
HIDDEN_SIZE = 256
NUM_LAYERS = 2

device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
print(f"ğŸš€ ì‚¬ìš© ì¥ì¹˜: {device}")

# ---------------------------------------------------------
# 2. ë°ì´í„° ì¤€ë¹„ (ì „ì²˜ë¦¬)
# ---------------------------------------------------------
print("ğŸ“š ë°ì´í„° ì½ëŠ” ì¤‘...")
try:
    with open('shakespeare.txt', 'r', encoding='utf-8') as f:
        text = f.read()
    print(f"âœ… ì „ì²´ ê¸€ì ìˆ˜: {len(text)}ì (ì´ì œ ë‹¤ ë¨¹ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤!)")
except FileNotFoundError:
    print("âŒ shakespeare.txt íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
    exit()

# ê¸€ì ì¡±ë³´ ë§Œë“¤ê¸°
chars = sorted(list(set(text)))
char_dic = {c: i for i, c in enumerate(chars)} # ê¸€ì -> ìˆ«ì
dic_size = len(chars)

print(f"ğŸ”¤ ë¬¸ì ì¢…ë¥˜: {dic_size}ê°œ")

# ---------------------------------------------------------
# 3. ë°°ì¹˜ë¥¼ ë§Œë“œëŠ” êµ­ì (Helper Function)
# ---------------------------------------------------------
# ì´ í•¨ìˆ˜ê°€ í•µì‹¬ì…ë‹ˆë‹¤! ì „ì²´ ë°ì´í„°ì—ì„œ ëœë¤ìœ¼ë¡œ 64ê°œ ì¡°ê°ì„ í¼ì˜µë‹ˆë‹¤.
def get_batch(text, batch_size, seq_length):
    input_batch = []
    target_batch = []
    
    for _ in range(batch_size):
        # 1. ëœë¤í•œ ìœ„ì¹˜ë¥¼ í•˜ë‚˜ ì°ìŒ
        start_idx = random.randint(0, len(text) - seq_length - 1)
        
        # 2. ê·¸ ìœ„ì¹˜ë¶€í„° ì •í•´ì§„ ê¸¸ì´ë§Œí¼ ì˜ë¼ëƒ„
        chunk = text[start_idx : start_idx + seq_length + 1]
        
        # 3. ìˆ«ìë¡œ ë³€í™˜
        encoded = [char_dic[c] for c in chunk]
        
        # 4. ë¬¸ì œ(Input)ì™€ ì •ë‹µ(Target) ë‚˜ëˆ„ê¸°
        # ë¬¸ì œ: H e l l o (ì• 5ê¸€ì)
        # ì •ë‹µ: e l l o ! (ë’¤ 5ê¸€ì - í•œ ì¹¸ ë°€ë¦¼)
        input_data = encoded[:-1]
        target_data = encoded[1:]
        
        input_batch.append(np.eye(dic_size)[input_data]) # One-hot Encoding
        target_batch.append(target_data)
        
    # íŒŒì´í† ì¹˜ í…ì„œë¡œ ë³€í™˜í•´ì„œ GPUë¡œ ë³´ëƒ„
    inputs = torch.tensor(input_batch, dtype=torch.float32).to(device)
    targets = torch.tensor(target_batch, dtype=torch.long).to(device)
    
    return inputs, targets

# ---------------------------------------------------------
# 4. ëª¨ë¸ ì„¤ê³„ (LSTM)
# ---------------------------------------------------------
class Net(nn.Module):
    def __init__(self, input_dim, hidden_dim, layers):
        super(Net, self).__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=layers, batch_first=True)
        self.fc = nn.Linear(hidden_dim, input_dim, bias=True)

    def forward(self, x):
        out, _ = self.lstm(x)
        # LSTM ê²°ê³¼ëŠ” 3Dì¸ë°, FCëŠ” 2Dë¥¼ ì›í•¨ -> ëª¨ì–‘ ë§ì¶”ê¸°
        out = out.reshape(-1, out.shape[2]) 
        out = self.fc(out)
        return out

model = Net(dic_size, HIDDEN_SIZE, NUM_LAYERS).to(device)

# ì†ì‹¤ í•¨ìˆ˜ì™€ ìµœì í™” ë„êµ¬
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.002)

# ---------------------------------------------------------
# 5. í•™ìŠµ ì‹œì‘ (Training)
# ---------------------------------------------------------
print("\nğŸ”¥ ìŠ¤íŒŒë¥´íƒ€ í›ˆë ¨ ì‹œì‘ (2000ë²ˆ ë°˜ë³µ)...")

for epoch in range(NUM_EPOCHS):
    # 1. êµ­ìë¡œ ë°ì´í„° í¼ì˜¤ê¸° (Batch)
    inputs, targets = get_batch(text, BATCH_SIZE, SEQ_LENGTH)
    
    # 2. ëª¨ë¸ ì˜ˆì¸¡
    outputs = model(inputs)
    
    # 3. ì˜¤ì°¨ ê³„ì‚° (ì •ë‹µì´ë‘ ì–¼ë§ˆë‚˜ í‹€ë ¸ë‚˜?)
    # targetsë¥¼ 1ì¤„ë¡œ ì­‰ í´ì•¼ í•¨
    loss = criterion(outputs, targets.view(-1))
    
    # 4. ìˆ˜ì • (ì—­ì „íŒŒ)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    # 200ë²ˆë§ˆë‹¤ ì„±ì í‘œ ì¶œë ¥
    if (epoch + 1) % 200 == 0:
        print(f"Epoch [{epoch+1}/{NUM_EPOCHS}], Loss: {loss.item():.4f}")

# ---------------------------------------------------------
# 6. ì €ì¥ (Save)
# ---------------------------------------------------------
print("\nğŸ’¾ ë˜‘ë˜‘í•´ì§„ ë‡Œ ì €ì¥ ì¤‘...")
save_data = {
    'model': model.state_dict(),
    'chars': chars,
    'hidden_size': HIDDEN_SIZE,
    'dic_size': dic_size,
    'num_layers': NUM_LAYERS # ì¸µ ê°œìˆ˜ë„ ì €ì¥í•´ì•¼ í•¨
}
torch.save(save_data, 'shakespeare.pt')
print("âœ… ì €ì¥ ì™„ë£Œ! (shakespeare.pt)")