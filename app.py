import streamlit as st
import torch
import torch.nn as nn
from torch.nn import functional as F
import numpy as np

# ---------------------------------------------------------
# 1. í˜ì´ì§€ ì„¤ì •
# ---------------------------------------------------------
st.set_page_config(page_title="Jay's Baby GPT", page_icon="ğŸ¤–", layout="wide")

st.markdown("""
<style>
    .main-text {
        font-family: 'Courier New', monospace;
        font-size: 1.1rem;
        line-height: 1.6;
        background-color: #f0f2f6;
        padding: 20px;
        border-radius: 10px;
        color: #1a1a1a;
    }
</style>
""", unsafe_allow_html=True)

# ---------------------------------------------------------
# 2. GPT ëª¨ë¸ êµ¬ì¡° (í•™ìŠµ ì½”ë“œì™€ ë˜‘ê°™ì´ ë³µì‚¬í•´ì•¼ í•¨)
# ---------------------------------------------------------
# ì„¤ì •ê°’ (í•™ìŠµí•  ë•Œ ì“´ ê²ƒê³¼ ë˜‘ê°™ì´ ë§ì¶°ì•¼ í•¨)
BLOCK_SIZE = 64
N_EMBD = 128
N_HEAD = 4
N_LAYER = 2
DROPOUT = 0.2
VOCAB_SIZE = 65 # ì…°ìµìŠ¤í”¼ì–´ ë°ì´í„° ë¬¸ì ê°œìˆ˜ (ëŒ€ëµ 65ê°œ)

class Head(nn.Module):
    def __init__(self, head_size):
        super().__init__()
        self.key = nn.Linear(N_EMBD, head_size, bias=False)
        self.query = nn.Linear(N_EMBD, head_size, bias=False)
        self.value = nn.Linear(N_EMBD, head_size, bias=False)
        self.register_buffer('tril', torch.tril(torch.ones(BLOCK_SIZE, BLOCK_SIZE)))
        self.dropout = nn.Dropout(DROPOUT)

    def forward(self, x):
        B,T,C = x.shape
        k = self.key(x)
        q = self.query(x)
        wei = q @ k.transpose(-2, -1) * C**-0.5
        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))
        wei = F.softmax(wei, dim=-1)
        wei = self.dropout(wei)
        v = self.value(x)
        out = wei @ v
        return out

class MultiHeadAttention(nn.Module):
    def __init__(self, num_heads, head_size):
        super().__init__()
        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])
        self.proj = nn.Linear(N_EMBD, N_EMBD)
        self.dropout = nn.Dropout(DROPOUT)

    def forward(self, x):
        out = torch.cat([h(x) for h in self.heads], dim=-1)
        out = self.dropout(self.proj(out))
        return out

class FeedFoward(nn.Module):
    def __init__(self, n_embd):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(n_embd, 4 * n_embd),
            nn.ReLU(),
            nn.Linear(4 * n_embd, n_embd),
            nn.Dropout(DROPOUT),
        )

    def forward(self, x):
        return self.net(x)

class Block(nn.Module):
    def __init__(self, n_embd, n_head):
        super().__init__()
        head_size = n_embd // n_head
        self.sa = MultiHeadAttention(n_head, head_size)
        self.ffwd = FeedFoward(n_embd)
        self.ln1 = nn.LayerNorm(n_embd)
        self.ln2 = nn.LayerNorm(n_embd)

    def forward(self, x):
        x = x + self.sa(self.ln1(x))
        x = x + self.ffwd(self.ln2(x))
        return x

class GPTLanguageModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.token_embedding_table = nn.Embedding(VOCAB_SIZE, N_EMBD)
        self.position_embedding_table = nn.Embedding(BLOCK_SIZE, N_EMBD)
        self.blocks = nn.Sequential(*[Block(N_EMBD, n_head=N_HEAD) for _ in range(N_LAYER)])
        self.ln_f = nn.LayerNorm(N_EMBD)
        self.lm_head = nn.Linear(N_EMBD, VOCAB_SIZE)

    def forward(self, idx, targets=None):
        B, T = idx.shape
        tok_emb = self.token_embedding_table(idx)
        pos_emb = self.position_embedding_table(torch.arange(T, device='cpu')) # CPUë¡œ ê°•ì œ
        x = tok_emb + pos_emb
        x = self.blocks(x)
        x = self.ln_f(x)
        logits = self.lm_head(x)
        return logits

# ---------------------------------------------------------
# 3. ëª¨ë¸ ë¡œë”© (GPT ì „ìš©)
# ---------------------------------------------------------
@st.cache_resource
def load_gpt_model():
    # 1. ê¹¡í†µ ëª¨ë¸ ë§Œë“¤ê¸°
    model = GPTLanguageModel()
    
    # 2. í•™ìŠµëœ ê°€ì¤‘ì¹˜(ê¸°ì–µ) ë¶ˆëŸ¬ì˜¤ê¸°
    try:
        # map_location='cpu' í•„ìˆ˜ (í´ë¼ìš°ë“œëŠ” GPUê°€ ì—†ì„ ìˆ˜ ìˆìŒ)
        state_dict = torch.load('baby_gpt.pt', map_location=torch.device('cpu'))
        model.load_state_dict(state_dict)
        model.eval()
    except Exception as e:
        return None, str(e)

    # 3. ë¬¸ì ì¡±ë³´(Vocab) ë§Œë“¤ê¸° (í•™ìŠµ ë•Œ ì“´ ê²ƒê³¼ ë˜‘ê°™ì•„ì•¼ í•¨)
    # ì…°ìµìŠ¤í”¼ì–´ ë°ì´í„°ì— ìˆëŠ” ëª¨ë“  ê¸€ì (ì´ 65ê°œ)
    chars = sorted(list(set("\n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz")))
    stoi = { ch:i for i,ch in enumerate(chars) }
    itos = { i:ch for i,ch in enumerate(chars) }
    
    return model, stoi, itos

model, stoi, itos = load_gpt_model()

# ---------------------------------------------------------
# 4. í™”ë©´ êµ¬ì„±
# ---------------------------------------------------------
with st.sidebar:
    st.title("ğŸ¤– Jay's Baby GPT")
    st.caption("Transformer Architecture (2017) êµ¬í˜„ì²´")
    st.markdown("---")
    temperature = st.slider("ì°½ì˜ì„± (Temperature)", 0.5, 1.5, 0.8)
    max_tokens = st.slider("ìƒì„± ê¸¸ì´", 100, 1000, 300)
    st.info("ì´ ëª¨ë¸ì€ ë¬¸ë§¥ì„ íŒŒì•…í•˜ëŠ” 'Attention' ë©”ì»¤ë‹ˆì¦˜ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")

st.title("ğŸ§  Baby GPT: The Beginning")
st.write("LSTM(ìˆœì°¨ ì²˜ë¦¬)ì„ ë„˜ì–´, **Transformer(ë³‘ë ¬ ì²˜ë¦¬)** ì‹œëŒ€ë¡œ ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤.")

col1, col2 = st.columns([1, 1])

with col1:
    start_str = st.text_input("ì²« ë¬¸ì¥ì„ ì…ë ¥í•˜ì„¸ìš”:", "The meaning of life is")
    btn = st.button("GPT, ìƒê°í•´ì„œ ê¸€ì„ ì¨ì¤˜!", type="primary")

with col2:
    if btn:
        if isinstance(model, str): # ì—ëŸ¬ ë©”ì‹œì§€ì¸ ê²½ìš°
            st.error(f"ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {model}\n'baby_gpt.pt' íŒŒì¼ì„ ì—…ë¡œë“œí–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        else:
            status = st.empty()
            progress = st.progress(0)
            
            # ì´ˆê¸° ì…ë ¥ê°’ ìˆ«ìë¡œ ë³€í™˜
            context = [stoi.get(c, 0) for c in start_str]
            idx = torch.tensor([context], dtype=torch.long)
            
            generated_text = start_str
            
            with torch.no_grad():
                for i in range(max_tokens):
                    # Context Window ìë¥´ê¸° (ìµœê·¼ 64ê¸€ìë§Œ ë´„)
                    idx_cond = idx[:, -BLOCK_SIZE:]
                    
                    # ì˜ˆì¸¡
                    logits = model(idx_cond)
                    logits = logits[:, -1, :] # ë§ˆì§€ë§‰ ê¸€ìë§Œ
                    
                    # í™•ë¥  ì¡°ì‘ (Temperature)
                    probs = F.softmax(logits / temperature, dim=-1)
                    
                    # ë‹¤ìŒ ê¸€ì ë½‘ê¸°
                    idx_next = torch.multinomial(probs, num_samples=1)
                    idx = torch.cat((idx, idx_next), dim=1)
                    
                    # ê²°ê³¼ ëˆ„ì 
                    next_char = itos[idx_next.item()]
                    generated_text += next_char
                    
                    # ë¡œë”©ë°”
                    if i % 10 == 0:
                        status.text(f"GPTê°€ ìƒê° ì¤‘... ({i}/{max_tokens})")
                        progress.progress((i+1)/max_tokens)
            
            status.empty()
            progress.empty()
            st.success("ìƒì„± ì™„ë£Œ!")
            st.markdown(f'<div class="main-text">{generated_text}</div>', unsafe_allow_html=True)